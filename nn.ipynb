{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34201c-39d5-43cd-b775-3d2daf22b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow.keras\n",
    "import PIL\n",
    "import pyvirtualdisplay\n",
    "\n",
    "from tensorflow import keras\n",
    "from matplotlib import animation\n",
    "from timeit import default_timer as timer\n",
    "from abc import ABC, abstractmethod\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential, load_model, clone_model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe386b-e52f-4e70-a56f-bf794857efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c0428-93fa-4de3-b9fe-9e80f7b962ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    @staticmethod\n",
    "    def forward_subs(A, b):\n",
    "        x = np.zeros(b.shape[0])\n",
    "        for i in range(0, x.shape[0]):\n",
    "            x[i] = b[i]\n",
    "            for j in range(0, i):\n",
    "                x[i] -= A[i][j] * x[j]\n",
    "            x[i] /= A[i][i]\n",
    "        return x\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward_subs(A, b):\n",
    "        x = np.zeros(b.shape[0])\n",
    "        for i in range(x.shape[0] - 1, -1, -1):\n",
    "            x[i] = b[i]\n",
    "            for j in range(x.shape[0] - 1, i, -1):\n",
    "                x[i] -= A[i][j] * x[j]\n",
    "            x[i] /= A[i][i]\n",
    "        return x\n",
    "        \n",
    "    @staticmethod\n",
    "    def incomplete_cholesky(A, epsilon = 1e-3):\n",
    "        dim = A.shape[0]\n",
    "        lower = np.zeros((dim, dim))\n",
    "        for i in range(0, dim):\n",
    "            for j in range(0, i + 1):\n",
    "                if A[i][j] <= epsilon:\n",
    "                    continue\n",
    "                sum = 0\n",
    "                for k in range(0, j):\n",
    "                    sum += lower[i][k] * lower[j][k]\n",
    "                lower[i][j] = A[i][j] - sum\n",
    "                if i == j:\n",
    "                    lower[i][j] = math.sqrt(lower[i][j])\n",
    "                else:\n",
    "                    lower[i][j] /= lower[j][j]\n",
    "        return lower\n",
    "        \n",
    "    @staticmethod\n",
    "    def cg_method(A, b, precond = None, epsilon = 1e-6):\n",
    "        epsilon /= b.shape[0]\n",
    "        x = np.zeros(b.shape[0])\n",
    "        r = np.copy(b)\n",
    "        if precond == 'cholesky':\n",
    "            l = Utility.incomplete_cholesky(A)\n",
    "            z = Utility.backward_subs(l.T, Utility.forward_subs(l, r))\n",
    "        else:\n",
    "            z = r\n",
    "        p = np.copy(z)\n",
    "        r_dot = np.dot(r, z)\n",
    "        while math.sqrt(r_dot) > epsilon:\n",
    "            p_A = np.dot(A, p)\n",
    "            alpha = r_dot / np.dot(p, p_A)\n",
    "            x = np.add(x, alpha * p)\n",
    "            r = np.subtract(r, alpha * p_A)\n",
    "            if precond == 'cholesky':\n",
    "                l = Utility.incomplete_cholesky(A)\n",
    "                z = Utility.backward_subs(l.T, Utility.forward_subs(l, r))\n",
    "            else:\n",
    "                z = r\n",
    "            beta = np.dot(r, z) / r_dot\n",
    "            r_dot = beta * r_dot\n",
    "            p = np.add(z, beta * p)\n",
    "        return x\n",
    "        \n",
    "    @staticmethod\n",
    "    def cgne_method(A, b, precond = None, epsilon = 1e-6):\n",
    "        At = A.T\n",
    "        return Utility.cg_method(np.dot(At, A), np.dot(At, b), precond, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740db94-6a9a-4f36-a25b-d75ae9af961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, dim, func='linear', train_bias=True):\n",
    "        self.__dim = dim\n",
    "        self.func = func\n",
    "        self.biases = np.zeros(dim)\n",
    "        self.train_bias = train_bias\n",
    "    \n",
    "    @classmethod\n",
    "    def from_biases(cls, biases, func = 'linear', train_bias=True):\n",
    "        layer = Layer(len(biases))\n",
    "        layer.biases = biases\n",
    "        layer.train_bias = train_bias\n",
    "        layer.func = func\n",
    "        return layer\n",
    "\n",
    "    def activate(self, x):\n",
    "        return Layer.activate_func(x, self.func)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return Layer.d_activate_func(x, self.func)\n",
    "            \n",
    "    def get_dim(self):\n",
    "        return self.__dim\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.biases) + ' -- ' + self.func\n",
    "\n",
    "    def set_to(self, target):\n",
    "        self.__dim = target.get_dim()\n",
    "        self.biases = np.copy(target.biases)\n",
    "        self.func = target.func\n",
    "\n",
    "    def copy(self):\n",
    "        return Layer.from_biases(np.copy(self.biases), self.func, self.train_bias)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add(x, y):\n",
    "        return x + y\n",
    "        \n",
    "    @staticmethod\n",
    "    def activate_func(x, func):\n",
    "        if func == 'relu': return np.maximum(0, x)\n",
    "        elif func == 'sigmoid': return 1 / (1 + np.exp(-x))\n",
    "        elif func == 'tanh': return np.tanh(x)\n",
    "        elif func == 'softmax' or func == 'softmax_d_entropy': \n",
    "            if x.ndim == 1:\n",
    "                return np.exp(x) / np.sum(np.exp(x), axis=-1)\n",
    "            else:\n",
    "                return np.einsum('ij,i->ij', np.exp(x), 1 / np.sum(np.exp(x), axis=-1))\n",
    "        else: return x\n",
    "        \n",
    "    @staticmethod\n",
    "    def d_activate_func(x, func):\n",
    "        if func == 'relu': \n",
    "            return np.where(x < 0, 0, 1)\n",
    "        elif func == 'sigmoid':\n",
    "            s = Layer.activate_func(x, func)\n",
    "            return (1 - s) * s\n",
    "        elif func == 'tanh': \n",
    "            return 1 - np.tanh(x) ** 2\n",
    "        elif func == 'softmax':\n",
    "            sm = Layer.activate_func(x, func)\n",
    "            if x.ndim == 1:\n",
    "                dx = np.einsum('i,j->ij', -sm, sm)\n",
    "                for i in range(0, len(x)):\n",
    "                    dx[i][i] += sm[i]\n",
    "            else:\n",
    "                dx = np.einsum('ij,ik->ijk', -sm, sm)\n",
    "                for i in range(0, x.shape[0]):\n",
    "                    for j in range(0, x.shape[1]):\n",
    "                        dx[i][j][j] += sm[i][j]\n",
    "            return dx    \n",
    "        elif func == 'softmax_d_entropy':\n",
    "            sm = Layer.activate_func(x, func)\n",
    "            if x.ndim == 1:\n",
    "                dx = np.repeat(np.expand_dims(-sm, axis=0), len(x), axis=0)\n",
    "                for i in range(0, len(x)):\n",
    "                    dx[i][i] += 1\n",
    "            else:\n",
    "                dx = np.repeat(np.expand_dims(-sm, axis=1), x.shape[1], axis=1)\n",
    "                for i in range(0, x.shape[0]):\n",
    "                    for j in range(0, x.shape[1]):\n",
    "                        dx[i][j][j] += 1\n",
    "            return dx    \n",
    "        else: \n",
    "            return np.ones(x.shape)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.layers_len = len(self.layers)\n",
    "        self.weights = [(np.random.rand(self.layers[i].get_dim(), self.layers[i + 1].get_dim())) *\n",
    "                        (math.sqrt(2 / (self.layers[i].get_dim() + self.layers[i + 1].get_dim()))) \n",
    "                        for i in range(0, self.layers_len - 1)]\n",
    "        self.db = [np.zeros(self.layers[i].get_dim()) \n",
    "                   for i in range(0, self.layers_len)]\n",
    "        self.dw = [np.zeros((self.layers[i].get_dim(), self.layers[i + 1].get_dim())) \n",
    "                   for i in range(0, self.layers_len - 1)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def get_out_dim(self):\n",
    "        return layers[-1].get_dim()\n",
    "        \n",
    "    def get_in_dim(self):\n",
    "        return layers[0].get_dim()\n",
    "\n",
    "    def set_var_to_tf_net(self, target):\n",
    "        for i in range(1, self.layers_len):\n",
    "            if not self.weights[i - 1].shape == target.layers[i - 1].get_weights()[0].shape:\n",
    "                raise Exception('weights shape mismatch')\n",
    "            if not self.layers[i].biases.shape == target.layers[i - 1].get_weights()[1].shape:\n",
    "                raise Exception('biases shape mismatch')\n",
    "\n",
    "            self.weights[i - 1] = np.copy(target.layers[i - 1].get_weights()[0])\n",
    "            self.layers[i].biases = np.copy(target.layers[i - 1].get_weights()[1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers[0].activate(x + self.layers[0].biases)\n",
    "        for i in range(1, self.layers_len):\n",
    "            x = self.layers[i].activate(np.matmul(x, self.weights[i - 1]) + self.layers[i].biases)\n",
    "        return x\n",
    "\n",
    "    def cache_forward(self, x):\n",
    "        x = x + self.layers[0].biases\n",
    "        z = [x]\n",
    "        x = self.layers[0].activate(x)\n",
    "        for i in range(1, self.layers_len):\n",
    "            x = np.matmul(x, self.weights[i - 1]) + self.layers[i].biases\n",
    "            z.append(x)\n",
    "            x = self.layers[i].activate(x)\n",
    "        return x, z\n",
    "            \n",
    "    def backward(self, loss, z):\n",
    "        layerIdx = self.layers_len - 1\n",
    "        df = self.layers[layerIdx].derivative(z[layerIdx])\n",
    "        if df.ndim > loss.ndim:\n",
    "            error = np.einsum('ijk,ik->ij', df, loss)\n",
    "        else:\n",
    "            error = np.multiply(df, loss)\n",
    "        self.db[layerIdx] = np.add(self.db[layerIdx], np.sum(error, axis=0))\n",
    "        layerIdx -= 1\n",
    "        while layerIdx >= 0:\n",
    "            self.dw[layerIdx] = np.add(self.dw[layerIdx], \n",
    "                                       np.einsum('ij,ik->kj', error, self.layers[layerIdx].activate(z[layerIdx])))\n",
    "            df = self.layers[layerIdx].derivative(z[layerIdx])\n",
    "            if df.ndim > error.ndim:\n",
    "                error = np.einsum('ij,kj->ik', error, self.weights[layerIdx])\n",
    "                error = np.einsum('ijk,ik->ij', df, error)\n",
    "            else:\n",
    "                error = np.einsum('ij,jk,ik->ij', df, self.weights[layerIdx], error)\n",
    "            self.db[layerIdx] = np.add(self.db[layerIdx], np.sum(error, axis=0))\n",
    "            layerIdx -= 1\n",
    "            \n",
    "    def clear_grad(self):\n",
    "        for i in range(0, self.layers_len):\n",
    "            for j in range(0, self.layers[i].get_dim()):\n",
    "                self.db[i][j] = 0\n",
    "        for i in range(0, self.layers_len - 1):\n",
    "            self.dw[i] = np.zeros((self.layers[i].get_dim(), self.layers[i + 1].get_dim()))\n",
    "\n",
    "    def set_to(self, target, set_grad=False):\n",
    "        self.layers[0].set_to(target.layers[0])\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].set_to(target.layers[i])\n",
    "            np.copyto(self.weights[i - 1], target.weights[i - 1])\n",
    "\n",
    "        if set_grad:\n",
    "            self.db = [np.copy(db) for db in target.db]\n",
    "            self.dw = [np.copy(dw) for db in target.dw]\n",
    "    \n",
    "    def copy(self, clear_grad=True):\n",
    "        copy = NeuralNetwork()\n",
    "        copy.layers_len = self.layers_len\n",
    "        copy.layers = [layer.copy() for layer in self.layers]\n",
    "        copy.weights = [np.copy(w) for w in self.weights]\n",
    "        \n",
    "        if clear_grad:\n",
    "            copy.db = [np.zeros(self.layers[i].get_dim()) \n",
    "                       for i in range(0, self.layers_len)]\n",
    "            copy.dw = [np.zeros((self.layers[i].get_dim(), self.layers[i + 1].get_dim())) \n",
    "                       for i in range(0, self.layers_len - 1)]\n",
    "        else:\n",
    "            copy.db = [np.copy(db) for db in self.db]\n",
    "            copy.dw = [np.copy(dw) for db in self.dw]\n",
    "\n",
    "        return copy\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        format = '{0}\\n{1}: {2}\\n'\n",
    "        out = '0: {0}\\n'.format(self.layers[0])\n",
    "        for i in range(1, self.layers_len):\n",
    "            out += format.format(self.weights[i - 1], i, self.layers[i])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "8e475848-7a12-4bc9-86a8-82cde788015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, nn, lr=0.001, weight_decay=0):\n",
    "        self.nn = nn\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def step(self, batch_size=1):\n",
    "        batch_scaler = 1.0 / batch_size\n",
    "        \n",
    "        if self.nn.layers[0].train_bias:\n",
    "            self.nn.layers[0].biases = np.subtract(self.nn.layers[0].biases, self.nn.db[0] * self.lr * batch_scaler)\n",
    "            \n",
    "        for i in range(1, self.nn.layers_len):\n",
    "            if self.nn.layers[i].train_bias:\n",
    "                self.nn.layers[i].biases = np.subtract(self.nn.layers[i].biases, self.nn.db[i] * self.lr * batch_scaler)\n",
    "            self.nn.weights[i - 1] = np.subtract(self.nn.weights[i - 1], self.nn.dw[i - 1] * self.lr * batch_scaler + self.nn.weights[i - 1] * self.weight_decay)\n",
    "\n",
    "    def clear(self):\n",
    "        return\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, nn, lr=0.01, b1=0.9, b2=0.999, weight_decay=0, epsilon=1e-7):\n",
    "        self.nn = nn\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.count = 1\n",
    "        self.m_b = [np.zeros(nn.layers[i].get_dim()) for i in range(0, self.nn.layers_len)]\n",
    "        self.v_b = [np.zeros(nn.layers[i].get_dim()) for i in range(0, self.nn.layers_len)]\n",
    "        self.m_w = [np.zeros((nn.layers[i].get_dim(), self.nn.layers[i + 1].get_dim()))\n",
    "                    for i in range(0, self.nn.layers_len - 1)]\n",
    "        self.v_w = [np.zeros((nn.layers[i].get_dim(), self.nn.layers[i + 1].get_dim()))\n",
    "                    for i in range(0, self.nn.layers_len - 1)]\n",
    "        \n",
    "    def step(self, batch_size=1):\n",
    "        batch_scaler = 1.0 / batch_size\n",
    "        coef = self.lr * math.sqrt(1 - self.b2 ** self.count) / (1 - self.b1 ** self.count)\n",
    "        \n",
    "        if self.nn.layers[0].train_bias:\n",
    "            self.m_b[0] = np.add(np.multiply(self.b1, self.m_b[0]), np.multiply(1 - self.b1, self.nn.db[0] * batch_scaler))\n",
    "            self.v_b[0] = np.add(np.multiply(self.b2, self.v_b[0]), np.multiply(1 - self.b2, (self.nn.db[0] * batch_scaler) ** 2))\n",
    "            self.nn.layers[0].biases = np.subtract(self.nn.layers[0].biases, \n",
    "                                                   coef * np.divide(self.m_b[0], np.sqrt(self.v_b[0])))\n",
    "                                                                    # out=self.m_b[0] * self.sqrt_inverse_epsilon,\n",
    "                                                                    # where=np.absolute(self.v_b[0])>self.sqr_epsilon))\n",
    "        \n",
    "        for i in range(1, self.nn.layers_len):\n",
    "            if self.nn.layers[i].train_bias:\n",
    "                self.m_b[i] = np.add(np.multiply(self.b1, self.m_b[i]), np.multiply(1 - self.b1, self.nn.db[i] * batch_scaler))\n",
    "                self.v_b[i] = np.add(np.multiply(self.b2, self.v_b[i]), np.multiply(1 - self.b2, (self.nn.db[i] * batch_scaler) ** 2))\n",
    "                self.nn.layers[i].biases = np.subtract(self.nn.layers[i].biases, \n",
    "                                                       coef * np.divide(self.m_b[i], np.sqrt(self.v_b[i])))\n",
    "                                                                        # out=self.m_b[i] * self.sqrt_inverse_epsilon,\n",
    "                                                                        # where=np.absolute(self.v_b[i])>self.sqr_epsilon))\n",
    "            self.m_w[i - 1] = np.add(np.multiply(self.b1, self.m_w[i - 1]), np.multiply(1 - self.b1, self.nn.dw[i - 1] * batch_scaler))\n",
    "            self.v_w[i - 1] = np.add(np.multiply(self.b2, self.v_w[i - 1]), np.multiply(1 - self.b2, (self.nn.dw[i - 1] * batch_scaler) ** 2))\n",
    "            self.nn.weights[i - 1] = np.subtract(self.nn.weights[i - 1], \n",
    "                                                 coef * np.divide(self.m_w[i - 1], np.sqrt(self.v_w[i - 1]))\n",
    "                                                                  # out=self.m_w[i - 1] * self.sqrt_inverse_epsilon,\n",
    "                                                                  # where=np.absolute(self.v_w[i - 1])>self.sqr_epsilon)\n",
    "                                                 + self.lr * self.weight_decay * self.nn.weights[i - 1])\n",
    "        self.count += 1\n",
    "        \n",
    "    def clear(self):\n",
    "        self.count = 1\n",
    "        for i in range(0, nn.layers_len):\n",
    "            for j in range(0, nn.layers[i].get_dim()):\n",
    "                self.m_b[i][j] = 0\n",
    "                self.v_b[i][j] = 0\n",
    "        for i in range(0, nn.layers_len - 1):\n",
    "            self.m_w[i] = np.zeros((nn.layers[i].get_dim(), self.nn.layers[i + 1].get_dim()))\n",
    "            self.v_w[i] = np.zeros((nn.layers[i].get_dim(), self.nn.layers[i + 1].get_dim()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8030a-6bd1-4393-b235-04b12811ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.mass = None\n",
    "\n",
    "    def append(self, item):\n",
    "        self.states.append(item[0])\n",
    "        self.actions.append(item[1])\n",
    "        self.rewards.append(item[2])\n",
    "        self.next_states.append(item[3])\n",
    "        self.dones.append(item[4])\n",
    "        \n",
    "        if len(self.states) > self.max_size:\n",
    "            self.leftpop()\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def sample(self, batch_size, item_wise=True):\n",
    "        n = len(self.states)\n",
    "        if item_wise:\n",
    "            items = [None] * batch_size\n",
    "            k = 0\n",
    "            for idx in np.random.randint(n, size=batch_size):\n",
    "                items[k] = (self.states[idx], self.actions[idx], self.rewards[idx], self.next_states[idx], self.dones[k], self.mass[idx])\n",
    "                k += 1\n",
    "            return items\n",
    "        else:\n",
    "            states, acts, rews, next_states, dones, mass = [None] * batch_size, [None] * batch_size, [None] * batch_size, [None] * batch_size, [None] * batch_size, [None] * batch_size\n",
    "            k = 0\n",
    "            for idx in np.random.randint(n, size=batch_size):\n",
    "                states[k] = self.states[idx]\n",
    "                acts[k] = self.actions[idx]\n",
    "                rews[k] = self.rewards[idx]\n",
    "                next_states[k] = self.next_states[idx]\n",
    "                dones[k] = self.dones[idx]\n",
    "                mass[k] = self.mass[idx]\n",
    "                k += 1\n",
    "            return states, acts, rews, next_states, dones, mass\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.mass = None\n",
    "    \n",
    "    def leftpop(self):  \n",
    "        self.states.pop(0)\n",
    "        self.actions.pop(0)\n",
    "        self.rewards.pop(0)\n",
    "        self.dones.pop(0)\n",
    "        self.next_states.pop(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0afd6-7c90-4aa9-8793-52fd5712acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, nn, optimizer, target_update_step, gamma=0.95, clipval=0):\n",
    "        self.main_net = nn\n",
    "        self.target_net = nn.copy(clear_grad=True)\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.target_update_step = target_update_step\n",
    "        self.clipval = clipval\n",
    "        self.__update_count = 0\n",
    "        \n",
    "    def train(self, states, acts, rews, next_states, dones, print_grad=False):\n",
    "        self.main_net.clear_grad()\n",
    "        out, z = self.main_net.cache_forward(states)\n",
    "        grad = 2 * np.array([o - rew - self.gamma * np.max(self.target_net(ns), axis=-1) if not done else o - rew\n",
    "                      for o, ns, rew, done in zip(out, next_states, rews, dones)])\n",
    "        if print_grad:\n",
    "            print((out, grad))\n",
    "        if self.clipval == 0: \n",
    "            self.main_net.backward(grad, z)\n",
    "        else:\n",
    "            self.main_net.backward(np.where(abs(grad) > self.clipval, np.sign(grad) * self.clipval, grad), z)\n",
    "        self.optimizer.step(len(states))\n",
    "\n",
    "        self.__update_count += 1\n",
    "        if self.__update_count >= self.target_update_step:\n",
    "            self.__update_count = 0\n",
    "            self.target_net.set_to(self.main_net)\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        return self.main_net(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0092d93-b5c9-4c2e-be22-2d9313909cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_DQN:\n",
    "    def __init__(self, nn, optimizer, target_update_step, gamma=0.95):\n",
    "        self.optimizer = optimizer\n",
    "        self.main_net = nn\n",
    "        self.target_net = clone_model(self.main_net)\n",
    "        self.target_net.set_weights(self.main_net.get_weights())\n",
    "        self.gamma = gamma\n",
    "        self.target_update_step = target_update_step\n",
    "        self.__input_shape = nn.input_shape\n",
    "        self.__output_shape = nn.output_shape\n",
    "        self.__update_count = 0\n",
    "        \n",
    "    def train(self, states, acts, rews, next_states, dones):\n",
    "        target_q_values = rews - self.gamma * tf.reduce_max(self.target_net.predict(states), axis=1)\n",
    "        masks = tf.one_hot(acts, self.__output_shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = tf.reduce_sum(tf.multiply(self.main_net(states), masks), axis=1)\n",
    "            loss = TF_DQN.loss_function(q_values, target_q_values)\n",
    "\n",
    "        grads = tape.gradient(loss, self.main_net.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, main_net.trainable_variables))\n",
    "\n",
    "        self.__update_count += 1\n",
    "        if self.__update_count >= self.target_update_step:\n",
    "            self.__update_count = 0\n",
    "            self.target_net.set_weights(self.main_net.get_weights())\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        return self.main_net(obs)\n",
    "\n",
    "    def loss_function(x, y):\n",
    "        return tf.math.square(tf.math.subtract(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4df2b-1028-4d3c-84ad-93760851d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAgent:\n",
    "    def __init__(self, env, policy, optimizer, gamma=0.95, value_func=None, epsilon=1, eps_decrementer=0.9995, min_epsilon=0.1, max_replay_batch=32, replay_buffer_size=20000):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_func = value_func\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decrementer = eps_decrementer\n",
    "        self.min_epsilon = min_epsilon\n",
    "        \n",
    "        self.state_num = 4\n",
    "        self.action_num = 2\n",
    "        \n",
    "        self.rb = ReplayBuffer(replay_buffer_size)\n",
    "        self.max_replay_batch = max_replay_batch\n",
    "\n",
    "    def run(self, eps=1, follow_policy_only=False, export_frames=False):\n",
    "        ep_num = 0\n",
    "        rets = []\n",
    "        if export_frames:\n",
    "            frames = []\n",
    "        \n",
    "        while ep_num < eps:\n",
    "            done = False\n",
    "            state, _ = env.reset()\n",
    "            step = 0\n",
    "            ret = 0\n",
    "            while not done:\n",
    "                act = self.get_action(state, follow_policy_only)\n",
    "                next_state, rew, done, truc, _ = env.step(act)\n",
    "                if export_frames:\n",
    "                    frames.append(env.render())\n",
    "                ret += rew\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                if step > max_steps:\n",
    "                    break\n",
    "            rets.append(ret)\n",
    "            ep_num += 1\n",
    "\n",
    "        if export_frames:\n",
    "            return rets, frames\n",
    "        \n",
    "        return rets\n",
    "    \n",
    "    def collect_experience(self, eps=1, follow_policy_only=False, export_frames=False):\n",
    "        ep_num = 0\n",
    "        if export_frames:\n",
    "            frames = []\n",
    "        \n",
    "        while ep_num < eps:\n",
    "            collect_start_idx = len(self.rb)\n",
    "            done = False\n",
    "            state, _ = env.reset()\n",
    "            step = 0\n",
    "            while not done:\n",
    "                act = self.get_action(state, follow_policy_only)\n",
    "                next_state, rew, done, truc, _ = env.step(act)\n",
    "                \n",
    "                if self.rb.append((state, act, rew, next_state, done)):\n",
    "                    collect_start_idx -= 1\n",
    "                state = next_state\n",
    "                if self.epsilon > self.min_epsilon:\n",
    "                    self.epsilon *= self.eps_decrementer\n",
    "                else: \n",
    "                    self.epsilon = self.min_epsilon\n",
    "                step += 1\n",
    "                if step > max_steps:\n",
    "                    break\n",
    "            ep_num += 1\n",
    "            rew_to_go = np.zeros(len(self.rb) - collect_start_idx)\n",
    "            rew_to_go[-1] = self.rb.rewards[-1]\n",
    "            for i in reversed(range(collect_start_idx, len(self.rb) - 1)):\n",
    "                rew_to_go[i - collect_start_idx] = self.rb.rewards[i] + rew_to_go[i + 1 - collect_start_idx] * self.gamma\n",
    "            if self.rb.mass == None:\n",
    "                self.rb.mass = []\n",
    "            rew_mean = np.mean(rew_to_go)\n",
    "            rew_istd = 1 / np.sqrt(np.var(rew_to_go))\n",
    "            rew_len = len(rew_to_go)\n",
    "            self.rb.mass.extend((rew_to_go - rew_mean) * rew_istd)\n",
    "            del self.rb.mass[:len(self.rb.mass) - len(self.rb)]\n",
    "\n",
    "            if export_frames:\n",
    "                return frames\n",
    "\n",
    "    def train(self, epochs=1):\n",
    "        if len(self.rb) < self.max_replay_batch:\n",
    "            return\n",
    "            \n",
    "        for epoch in range(0, epochs):\n",
    "            states, acts, rews, next_states, dones, mass = self.rb.sample(self.max_replay_batch, item_wise=False)\n",
    "            policy.clear_grad()\n",
    "            out, z = self.policy.cache_forward(states)\n",
    "            loss = -np.array([[0, m] if a == 1 else [m, 0] for m, a in zip(mass, acts)])\n",
    "            policy.backward(loss, z)\n",
    "            optimizer.step(batch_size=self.max_replay_batch)\n",
    "    \n",
    "    def get_action(self, state, follow_policy_only=False):\n",
    "        if follow_policy_only:\n",
    "            return np.argmax(self.policy(state))\n",
    "            \n",
    "        if state.ndim == 1:\n",
    "            rand = np.random.rand()\n",
    "            return np.random.choice(self.action_num, p=self.policy(state)) if rand > self.epsilon else self.rand_act()\n",
    "            \n",
    "        actions = np.zeros(state.shape[0])\n",
    "        rand = np.random.rand(state.shape[0])\n",
    "        for i in range(0, len(rand)):\n",
    "            if rand[i] > self.epsilon:\n",
    "                actions[i] = np.random.choice(self.action_num, p=self.policy(state[i]))\n",
    "            else:\n",
    "                actions[i] = self.rand_act()\n",
    "        return actions\n",
    "\n",
    "    def rand_act(self):\n",
    "        return np.random.choice(self.action_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c41c1a-c878-4ec0-82f4-9de236cc5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFCartPoleAgent:\n",
    "    def __init__(self, env, policy, optimizer, gamma=0.95, value_func=None, epsilon=1, eps_decrementer=0.9995, min_epsilon=0.01, max_replay_batch=32, replay_buffer_size=20000):\n",
    "        pass\n",
    "\n",
    "    def run(self, eps=1, follow_policy_only=False, export_frames=False):\n",
    "        pass\n",
    "    \n",
    "    def collect_experience(self, eps=1, follow_policy_only=False, export_frames=False):\n",
    "        pass\n",
    "\n",
    "    def train(self, epochs=1):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state, follow_policy_only=False):\n",
    "        pass\n",
    "\n",
    "    def rand_act(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef3012-4f9a-4b07-9891-a829c2ffac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, filename='gym_anim.gif', path='C:\\\\Users\\\\mypc\\\\Documents\\\\Jupyterlab\\\\cart-pole\\\\'):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c216828-cc72-4f28-b745-62eb096987fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env.reset()\n",
    "policy = NeuralNetwork(\n",
    "    Layer(env.observation_space.shape[0], train_bias=False),\n",
    "    Layer(12),\n",
    "    Layer(12),\n",
    "    Layer(2, 'softmax_d_entropy')\n",
    ")\n",
    "# optimizer = SGD(policy, weight_decay=0, lr=0.01)\n",
    "optimizer = Adam(policy, weight_decay=0.000001, lr=0.01)\n",
    "agent = CartPoleAgent(env, policy, optimizer, max_replay_batch=32, replay_buffer_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626dfec8-57cc-494f-be3e-b15b86a1664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rb.clear()\n",
    "agent.collect_experience(1, follow_policy_only=True)\n",
    "sum(agent.rb.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c489a-7063-429f-8f1d-335f247e704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble_steps = 5\n",
    "train_steps = 1\n",
    "export_num = 5\n",
    "avg_rets = []\n",
    "log_vid = False\n",
    "\n",
    "agent.rb.clear()\n",
    "agent.collect_experience(preamble_steps)\n",
    "\n",
    "if log_vid:\n",
    "    rets, frames = agent.run(1, follow_policy_only=True, export_frames=True)\n",
    "    save_frames_as_gif(frames, filename='cart-pole-ep-0.gif')\n",
    "else:\n",
    "    rets = agent.run(1, follow_policy_only=True, export_frames=False)\n",
    "\n",
    "for _ in range(train_steps):\n",
    "    agent.collect_experience(1)\n",
    "    agent.train(5)\n",
    "    \n",
    "    if (_ + 1) % (train_steps / min(train_steps, 500)) == 0:\n",
    "        avg_rets.append(np.sum(agent.run(6, follow_policy_only=True)) / 6)\n",
    "\n",
    "    if log_vid:\n",
    "        if (_ + 1) % (train_steps / export_num) == 0:\n",
    "            rets, frames = agent.run(1, follow_policy_only=True, export_frames=True)\n",
    "            save_frames_as_gif(frames, filename='cart-pole-ep-{0}.gif'.format(_ + 1))\n",
    "            print('rendered')\n",
    "    \n",
    "    if (_ + 1) % (train_steps / 10) == 0:\n",
    "        print(str(_ + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa6daf-70dc-44e5-a639-2a1d5a2e928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe8d75-fce6-4c6c-a4bf-3a2cd0761c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(rets, x_up_bound=1, x_low_bound=0, tick_num=5):\n",
    "    plt.plot(rets)\n",
    "    plt.title('Performance of RL Policy')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.xticks(ticks=np.arange(0, len(rets) + 1, len(rets) / tick_num, dtype=int), \n",
    "               labels=np.arange(x_low_bound, x_up_bound + 1, (x_up_bound - x_low_bound) / tick_num, dtype=int))\n",
    "    plt.ylim([0, max_steps + 10])\n",
    "    plt.show()\n",
    "    \n",
    "plot(avg_rets, train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fcacb-0e06-4eab-a261-f65624d562bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.rb.clear()\n",
    "agent.collect_experience(1, follow_policy_only=True)\n",
    "np.sum(agent.rb.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08af241-cb0d-44fb-9c11-025c4caea9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(\n",
    "    Layer(env.observation_space.shape[0], train_bias=False),\n",
    "    Layer(4, 'relu'),\n",
    "    Layer(4, 'relu'),\n",
    "    Layer(1)\n",
    ")\n",
    "dqn_opt = SGD(net, lr=0.001)\n",
    "dqn = DQN(net, dqn_opt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b81f0-413d-421e-b114-faa0a110aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_net = Sequential([\n",
    "    keras.layers.Input(shape=env.observation_space.shape),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "tf_dqn_opt = keras.optimizers.SGD(learning_rate=0.001)\n",
    "tf_dqn = TF_DQN(tf_net, tf_dqn_opt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b9130-38f4-4e86-87f8-7cd2e64b1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_net.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc61c4-5975-4161-a4f1-1b18f23e5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array(agent.rb.sample(1, item_wise=False)[0])\n",
    "tf_dqn(sample)\n",
    "states, acts, rews, next_states, dones, mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5334ec-e0b6-4cab-80c0-4b970851ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_net.get_weights())\n",
    "sample_states, sample_acts, sample_rews, sample_next_states, sample_dones, sample_mass = agent.rb.sample(1, item_wise=False)\n",
    "sample_states = np.array(sample_states)\n",
    "sample_next_states = np.array(sample_next_states)\n",
    "sample_acts = tf.convert_to_tensor(sample_acts)\n",
    "\n",
    "dqn.train(sample_states, sample_acts, sample_rews, sample_next_states, sample_dones)\n",
    "tf_dqn.train(sample_states, sample_acts, sample_rews, sample_next_states, sample_dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea8517-8ce9-4687-8764-d00d97ee308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "print(tf_net.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454c69d-35e7-4ee5-a9df-f3b84b8d9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(net)\n",
    "dqn_train_steps = 300000\n",
    "\n",
    "for _ in range(dqn_train_steps):\n",
    "    sample = agent.rb.sample(1, item_wise=False)[:-1]\n",
    "    dqn.train(*sample, print_grad=False)\n",
    "    \n",
    "    if (_ + 1) % (dqn_train_steps / 10) == 0:\n",
    "        print(str(_ + 1))\n",
    "        \n",
    "    if (_ + 1) % 5000 == 0:\n",
    "        agent.rb.clear()\n",
    "        agent.collect_experience(1, follow_policy_only=True)\n",
    "# print(net)\n",
    "# dqn(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcd6b1-76ac-4715-bccb-57442b87fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_steps = []\n",
    "for idx, x in enumerate(agent.rb.dones):\n",
    "    if x:\n",
    "        done_steps.append(idx)\n",
    "if done_steps == []:\n",
    "    done_steps.append(len(agent.rb.dones) - 1)\n",
    "done_steps.insert(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "7d1d6482-77c5-4402-bd09-c2b222c703df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'done_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[884], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(dqn(agent\u001b[38;5;241m.\u001b[39mrb\u001b[38;5;241m.\u001b[39mstates[\u001b[43mdone_steps\u001b[49m[ep] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:done_steps[ep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]]))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimesteps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'done_steps' is not defined"
     ]
    }
   ],
   "source": [
    "ep = 0\n",
    "plt.plot(dqn(agent.rb.states[done_steps[ep] + 1:done_steps[ep + 1]]))\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Value')\n",
    "plt.ylim(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "9188d41c-14a1-4627-b43f-59e7a8b146d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[3, 2, 3, 5]]\n",
    "y = [[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "6e5903e5-2e14-4487-ac01-10bdd643e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = Sequential([Dense(4, input_shape=(4,)), Dense(2, activation='softmax')])\n",
    "tf_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    Layer(4, train_bias=False),\n",
    "    Layer(4),\n",
    "    Layer(2, func='softmax')\n",
    ")\n",
    "model_opt = Adam(model, lr=0.01, weight_decay=0)\n",
    "model.set_var_to_tf_net(tf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "f5da8de7-9b08-4dd4-b6df-74dcbcf5344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [0.000 0.000 0.000 0.000] -- linear\n",
      "[[0.565 0.278 0.743 -0.349]\n",
      " [-0.832 0.121 -0.678 -0.400]\n",
      " [-0.802 -0.536 0.748 -0.291]\n",
      " [-0.391 0.176 -0.460 -0.128]]\n",
      "1: [-0.118 0.100 0.097 -0.104] -- linear\n",
      "[[0.804 0.467]\n",
      " [-0.413 0.335]\n",
      " [-0.050 0.152]\n",
      " [0.660 -0.674]]\n",
      "2: [-0.103 0.103] -- softmax\n",
      "\n",
      "[array([[0.565, 0.278, 0.743, -0.349],\n",
      "       [-0.832, 0.121, -0.678, -0.400],\n",
      "       [-0.802, -0.536, 0.748, -0.291],\n",
      "       [-0.391, 0.176, -0.460, -0.128]], dtype=float32), array([-0.118, 0.100, 0.097, -0.104], dtype=float32)]\n",
      "[array([[0.804, 0.467],\n",
      "       [-0.413, 0.335],\n",
      "       [-0.050, 0.152],\n",
      "       [0.660, -0.674]], dtype=float32), array([-0.103, 0.103], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(500):\n",
    "    model.clear_grad()\n",
    "    out, z = model.cache_forward(x)\n",
    "    loss = (out - y)\n",
    "    model.backward(loss, z)\n",
    "    model_opt.step()\n",
    "    \n",
    "    tf_model.fit(x, y, verbose='0')\n",
    "\n",
    "print(model)\n",
    "for layer in tf_model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d69887-21a5-4aa0-bcbf-e608fe98eed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
